

import time
import pymongo
import pandas as pd
from datetime import datetime
from pymongo import MongoClient
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer

from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# ============ üîê MongoDB CONFIG ============
MONGODB_URI = "mongodb+srv://hieuducle:255381228@chat-rag.qsimdqe.mongodb.net/?retryWrites=true&w=majority&appName=chat-rag"
DB_NAME = "chat-rag"
COLLECTION_NAME = "grocery"

# ============ üí° Embedding Model ============
embedding_model = SentenceTransformer("keepitreal/vietnamese-sbert")

def get_embedding(text: str) -> list[float]:
    if not text.strip():
        print("Attempted to get embedding for empty text.")
        return []
    embedding = embedding_model.encode(text)
    return embedding.tolist()

# ============ Crawl using Selenium ============
def crawl_article(url):
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(options=chrome_options)

    try:
        driver.get(url)
        time.sleep(3)  

        soup = BeautifulSoup(driver.page_source, "html.parser")

        title = soup.find("div", class_="product-title").find("h1").text.strip()
        brand = soup.find("span", class_="product-type").find("b").text.strip()
        status = soup.find("span", class_="product-status").find("b").text.strip()

        price_tag = soup.select_one("#price-preview .pro-price")
        if price_tag:
            price = price_tag.text.strip()
            clean_price = int(price.replace(",", "").replace(".", "").replace("‚Ç´", "").strip())
            print(f"‚úÖ Gi√°: {price} ‚Üí {clean_price}")
        else:
            print("Kh√¥ng t√¨m th·∫•y gi√°.")
            price, clean_price = "", 0

        # M√¥ t·∫£ s·∫£n ph·∫©m
        desc_div = soup.find("div", class_="product-short-desc")
        description = ""
        if desc_div:
            li_items = desc_div.find_all("li")
            if li_items:
                description = "\n".join(li.get_text(strip=True) for li in li_items)
            else:
                span = desc_div.find("span")
                if span and "<br" in str(span):
                    lines = str(span).split("<br>")
                    description = "\n".join(BeautifulSoup(line, "html.parser").get_text(strip=True) for line in lines)
                else:
                    p_tags = desc_div.find_all("p")
                    description = "\n".join(p.get_text(strip=True) for p in p_tags if p.get_text(strip=True))

        # ·∫¢nh s·∫£n ph·∫©m
        img_tag = soup.find("img", class_="product-image-feature")
        if img_tag:
            if img_tag["src"].startswith("//"):
                img_url = "https:" + img_tag["src"]
            else:
                img_url = img_tag["src"]
        else:
            img_url = ""

        return {
            "name_title": title,
            "image": img_url,
            "status": status,
            "brand": brand,
            "description": description,
            "source_url": url,
            "price_display": price,
            "price_value": clean_price,
            "timestamp": datetime.utcnow()
        }

    except Exception as e:
        print(f"L·ªói khi crawl {url}: {e}")
        return None

    finally:
        driver.quit()

def save_to_mongodb(data_list):
    try:
        client = MongoClient(MONGODB_URI)
        db = client[DB_NAME]
        collection = db[COLLECTION_NAME]
        print(" K·∫øt n·ªëi MongoDB th√†nh c√¥ng")

        if data_list:
            collection.insert_many(data_list)
            print(f"ƒê√£ l∆∞u {len(data_list)} s·∫£n ph·∫©m v√†o MongoDB.")
        else:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u.")
    except Exception as e:
        print("L·ªói khi l∆∞u v√†o MongoDB:", e)


if __name__ == "__main__":
    # all_data = []
    # try:
    #     with open("urls.txt", "r", encoding="utf-8") as f:
    #         urls = [line.strip() for line in f if line.strip()]
    # except FileNotFoundError:
    #     print("‚ùå File urls.txt kh√¥ng t·ªìn t·∫°i.")
    #     urls = []

    # for i, url in enumerate(urls, 1):
    #     print(f"\nüîç [{i}/{len(urls)}] ƒêang x·ª≠ l√Ω: {url}")
    #     result = crawl_article(url)
    #     if result:
    #         all_data.append(result)

    # if all_data:
    #     save_to_mongodb(all_data)
    # else:
    #     print("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë∆∞·ª£c crawl.")


    #  Connect to MongoDB
    client = pymongo.MongoClient(MONGODB_URI) 
    db = client[DB_NAME]  
    collection = db["grocery"] 
    data = list(collection.find())

   
    df = pd.DataFrame(data)
    df["embedding"] = df["name_title"].apply(get_embedding)
    collection = db['embedding_for_vector_search_new']
    collection.delete_many({})
    documents = df.to_dict("records")
    collection.insert_many(documents)

    print("Data ingestion into MongoDB completed")


